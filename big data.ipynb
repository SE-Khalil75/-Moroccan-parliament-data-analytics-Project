{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354a95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa16db",
   "metadata": {},
   "source": [
    "# Les representants#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed8e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Adil Dfouf\n",
      "Party: Parti authenticité et modernité\n",
      "Commission: Adil Dfouf\n",
      "Group: Commission des secteurs productifs\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2903-1638961006.jpg?itok=HfBJCMjS\n",
      "\n",
      "\n",
      "Name: Mohamed Hmami\n",
      "Party: Parti de l'istiqlal\n",
      "Commission: Mohamed Hmami\n",
      "Group: Commission des finances et du développement économique\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2904-1638961219.jpg?itok=v2JFVhFw\n",
      "\n",
      "\n",
      "Name: Mohamed Zemmouri\n",
      "Party: Union constitutionnelle\n",
      "Commission: Mohamed Zemmouri\n",
      "Group: Commission de l'intérieur, des collectivités territoriales, de l'habitat et de la politique de la ville\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2906-1649324685.jpg?itok=Ot0EA4v7\n",
      "\n",
      "\n",
      "Name: Abdelkader Taher\n",
      "Party: Union socialiste des forces populaires\n",
      "Commission: Abdelkader Taher\n",
      "Group: Commission des finances et du développement économique\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2907-1638961379.jpg?itok=r8Kvi6Lz\n",
      "\n",
      "\n",
      "Name: Mohamed Larbi Merabet\n",
      "Party: Parti authenticité et modernité\n",
      "Commission: Mohamed Larbi Merabet\n",
      "Group: Commission de l'intérieur, des collectivités territoriales, de l'habitat et de la politique de la ville\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2908-1638961803.jpg?itok=N0TRG08o\n",
      "\n",
      "\n",
      "Name: Abdennour Hasnaoui\n",
      "Party: Union socialiste des forces populaires\n",
      "Commission: Abdennour Hasnaoui\n",
      "Group: Commission des secteurs productifs\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2909-1638961957.jpg?itok=KT6IZv-D\n",
      "\n",
      "\n",
      "Name: Mohamed Elarbi Ahnin\n",
      "Party: Parti authenticité et modernité\n",
      "Commission: Mohamed Elarbi Ahnin\n",
      "Group: Commission des affaires étrangères, de la défense nationale, des affaires islamiques et des MRE\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2910-1636373767.jpg?itok=SDidW0g3\n",
      "\n",
      "\n",
      "Name: Monssef Toub\n",
      "Party: Parti de l'istiqlal\n",
      "Commission: Monssef Toub\n",
      "Group: Commission des secteurs productifs\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2911-1638962287.jpg?itok=hGdbkceW\n",
      "\n",
      "\n",
      "Name: Rachid Talbi El Alami\n",
      "Party: Rassemblement national des indépendants\n",
      "Commission: Rachid Talbi El Alami\n",
      "Group: Commission de justice, de législation et des droits de l'homme\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2912-1640010675.jpg?itok=11BzQj9W\n",
      "\n",
      "\n",
      "Name: Nor-Ddin Elhrouchi\n",
      "Party: Union constitutionnelle\n",
      "Commission: Nor-Ddin Elhrouchi\n",
      "Group: Commission de l'intérieur, des collectivités territoriales, de l'habitat et de la politique de la ville\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/11_new_s_1.jpg?itok=qaQRqdH4\n",
      "\n",
      "\n",
      "Name: Hamid Darrak\n",
      "Party: Union socialiste des forces populaires\n",
      "Commission: Hamid Darrak\n",
      "Group: Commission du contrôle des finances publiques\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2914-1635939007.jpg?itok=OrRnyily\n",
      "\n",
      "\n",
      "Name: Idriss Sauir Mansouri\n",
      "Party: Parti de l'istiqlal\n",
      "Commission: Idriss Sauir Mansouri\n",
      "Group: Commission des secteurs productifs\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2915-1638962439.jpg?itok=Rux3yfuX\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.chambredesrepresentants.ma/fr/%D8%AF%D9%84%D9%8A%D9%84-%D8%A3%D8%B9%D8%B6%D8%A7%D8%A1-%D9%85%D8%AC%D9%84%D8%B3-%D8%A7%D9%84%D9%86%D9%88%D8%A7%D8%A8/2021-2026'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the div elements with class \"col-sm-6 col-lg-3 mb-5\"\n",
    "    div_elements = soup.find_all('div', class_='col-sm-6 col-lg-3 mb-5')\n",
    "\n",
    "    # Loop through the div elements and extract the data\n",
    "    for div in div_elements:\n",
    "        # Extract the image source URL\n",
    "        img_src = div.find('img')['src']\n",
    "\n",
    "        # Extract the representative's name\n",
    "        name = div.find('span', class_='q-name').text.strip()\n",
    "\n",
    "        # Extract the political party\n",
    "        party = div.find('span', text='Parti authenticité et modernité')\n",
    "        if party:\n",
    "            party = party.text.strip()\n",
    "        else:\n",
    "            party = div.find('span', class_='q-name').find_next('span').text.strip()\n",
    "\n",
    "        # Extract the commission and group information\n",
    "        commission = div.find('a', href=True, text=True).text.strip()\n",
    "        group = div.find('a', href=True, text=True).find_next('a', href=True, text=True).text.strip()\n",
    "\n",
    "        # Print the extracted data\n",
    "        print(\"Name:\", name)\n",
    "        print(\"Party:\", party)\n",
    "        print(\"Commission:\", commission)\n",
    "        print(\"Group:\", group)\n",
    "        print(\"Image URL:\", img_src)\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51033be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Adil Dfouf\n",
      "Party: Parti authenticité et modernité\n",
      "Commission: Adil Dfouf\n",
      "Group: Commission des secteurs productifs\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2903-1638961006.jpg?itok=HfBJCMjS\n",
      "\n",
      "\n",
      "Name: Mohamed Hmami\n",
      "Party: Parti de l'istiqlal\n",
      "Commission: Mohamed Hmami\n",
      "Group: Commission des finances et du développement économique\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2904-1638961219.jpg?itok=v2JFVhFw\n",
      "\n",
      "\n",
      "Name: Mohamed Zemmouri\n",
      "Party: Union constitutionnelle\n",
      "Commission: Mohamed Zemmouri\n",
      "Group: Commission de l'intérieur, des collectivités territoriales, de l'habitat et de la politique de la ville\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2906-1649324685.jpg?itok=Ot0EA4v7\n",
      "\n",
      "\n",
      "Name: Abdelkader Taher\n",
      "Party: Union socialiste des forces populaires\n",
      "Commission: Abdelkader Taher\n",
      "Group: Commission des finances et du développement économique\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2907-1638961379.jpg?itok=r8Kvi6Lz\n",
      "\n",
      "\n",
      "Name: Mohamed Larbi Merabet\n",
      "Party: Parti authenticité et modernité\n",
      "Commission: Mohamed Larbi Merabet\n",
      "Group: Commission de l'intérieur, des collectivités territoriales, de l'habitat et de la politique de la ville\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2908-1638961803.jpg?itok=N0TRG08o\n",
      "\n",
      "\n",
      "Name: Abdennour Hasnaoui\n",
      "Party: Union socialiste des forces populaires\n",
      "Commission: Abdennour Hasnaoui\n",
      "Group: Commission des secteurs productifs\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2909-1638961957.jpg?itok=KT6IZv-D\n",
      "\n",
      "\n",
      "Name: Mohamed Elarbi Ahnin\n",
      "Party: Parti authenticité et modernité\n",
      "Commission: Mohamed Elarbi Ahnin\n",
      "Group: Commission des affaires étrangères, de la défense nationale, des affaires islamiques et des MRE\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2910-1636373767.jpg?itok=SDidW0g3\n",
      "\n",
      "\n",
      "Name: Monssef Toub\n",
      "Party: Parti de l'istiqlal\n",
      "Commission: Monssef Toub\n",
      "Group: Commission des secteurs productifs\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2911-1638962287.jpg?itok=hGdbkceW\n",
      "\n",
      "\n",
      "Name: Rachid Talbi El Alami\n",
      "Party: Rassemblement national des indépendants\n",
      "Commission: Rachid Talbi El Alami\n",
      "Group: Commission de justice, de législation et des droits de l'homme\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2912-1640010675.jpg?itok=11BzQj9W\n",
      "\n",
      "\n",
      "Name: Nor-Ddin Elhrouchi\n",
      "Party: Union constitutionnelle\n",
      "Commission: Nor-Ddin Elhrouchi\n",
      "Group: Commission de l'intérieur, des collectivités territoriales, de l'habitat et de la politique de la ville\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/11_new_s_1.jpg?itok=qaQRqdH4\n",
      "\n",
      "\n",
      "Name: Hamid Darrak\n",
      "Party: Union socialiste des forces populaires\n",
      "Commission: Hamid Darrak\n",
      "Group: Commission du contrôle des finances publiques\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2914-1635939007.jpg?itok=OrRnyily\n",
      "\n",
      "\n",
      "Name: Idriss Sauir Mansouri\n",
      "Party: Parti de l'istiqlal\n",
      "Commission: Idriss Sauir Mansouri\n",
      "Group: Commission des secteurs productifs\n",
      "Image URL: https://www.chambredesrepresentants.ma/sites/default/files/styles/thumbnail/public/pictures/picture-2915-1638962439.jpg?itok=Rux3yfuX\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the div elements with class \"col-sm-6 col-lg-3 mb-5\"\n",
    "    div_elements = soup.find_all('div', class_='col-sm-6 col-lg-3 mb-5')\n",
    "\n",
    "    # Loop through the div elements and extract the data\n",
    "    for div in div_elements:\n",
    "        # Extract the image source URL\n",
    "        img_src = div.find('img')['src']\n",
    "\n",
    "        # Extract the representative's name\n",
    "        name = div.find('span', class_='q-name').text.strip()\n",
    "\n",
    "        # Extract the political party\n",
    "        party = div.find('span', text='Parti authenticité et modernité')\n",
    "        if party:\n",
    "            party = party.text.strip()\n",
    "        else:\n",
    "            party = div.find('span', class_='q-name').find_next('span').text.strip()\n",
    "\n",
    "        # Extract the commission and group information\n",
    "        commission = div.find('a', href=True, text=True).text.strip()\n",
    "        group = div.find('a', href=True, text=True).find_next('a', href=True, text=True).text.strip()\n",
    "\n",
    "        # Print the extracted data\n",
    "        print(\"Name:\", name)\n",
    "        print(\"Party:\", party)\n",
    "        print(\"Commission:\", commission)\n",
    "        print(\"Group:\", group)\n",
    "        print(\"Image URL:\", img_src)\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "393b7f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully scraped and saved to representatives.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url_template = \"https://www.chambredesrepresentants.ma/fr/%D8%AF%D9%84%D9%8A%D9%84-%D8%A3%D8%B9%D8%B6%D8%A7%D8%A1-%D9%85%D8%AC%D9%84%D8%B3-%D8%A7%D9%84%D9%86%D9%88%D8%A7%D8%A8/2021-2026?page={}\"\n",
    "\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        members_divs = soup.find_all('div', class_='col-sm-6 col-lg-3 mb-5')\n",
    "\n",
    "        data = []\n",
    "        for div in members_divs:\n",
    "            name_element = div.find('span', class_='q-name')\n",
    "\n",
    "            commission_element = div.find('a', href=lambda href: href and 'commission' in href)\n",
    "            group_element = div.find('a', href=lambda href: href and 'groupe' in href)\n",
    "\n",
    "            name = name_element.text.strip() if name_element else 'N/A'\n",
    "            commission = commission_element.text.strip() if commission_element else 'N/A'\n",
    "            group = group_element.text.strip() if group_element else 'N/A'\n",
    "\n",
    "            # Extract the political party\n",
    "            party = div.find('span', text='Parti authenticité et modernité')\n",
    "            if party:\n",
    "                party = party.text.strip()\n",
    "            else:\n",
    "                party = div.find('span', class_='q-name').find_next('span').text.strip()\n",
    "\n",
    "            data.append([name, party, commission, group])\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Scrape multiple pages\n",
    "total_pages = 32  # You can set the number of pages you want to scrape\n",
    "all_data = []\n",
    "for page in range(1, total_pages + 1):\n",
    "    url = url_template.format(page)\n",
    "    page_data = scrape_page(url)\n",
    "    if page_data:\n",
    "        all_data.extend(page_data)\n",
    "\n",
    "# Write data to CSV file\n",
    "csv_file_path = 'representatives.csv'\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Name', 'Party', 'Commission', 'Group'])\n",
    "    csv_writer.writerows(all_data)\n",
    "\n",
    "print(f\"Data has been successfully scraped and saved to {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87cf5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and stored in 'representatives.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL of the webpage to scrape\n",
    "base_url = 'https://www.chambredesrepresentants.ma/fr/%D8%AF%D9%84%D9%8A%D9%84-%D8%A3%D8%B9%D8%B6%D8%A7%D8%A1-%D9%85%D8%AC%D9%84%D8%B3-%D8%A7%D9%84%D9%86%D9%88%D8%A7%D8%A8/2021-2026?page='\n",
    "\n",
    "# Number of pages to scrape\n",
    "num_pages = 33  # Change this to the number of pages you want to scrape\n",
    "\n",
    "# Create and open a CSV file for writing\n",
    "with open('representatives.csv', mode='w', newline='') as csv_file:\n",
    "    # Define the CSV writer\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write the header row to the CSV file\n",
    "    csv_writer.writerow(['Name', 'Party', 'Commission', 'Group'])\n",
    "\n",
    "    # Loop through multiple pages\n",
    "    for page_number in range(1, num_pages + 1):\n",
    "        # Construct the URL for the current page\n",
    "        url = f'{base_url}{page_number}'\n",
    "\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the webpage using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all the div elements with class \"col-sm-6 col-lg-3 mb-5\"\n",
    "            div_elements = soup.find_all('div', class_='col-sm-6 col-lg-3 mb-5')\n",
    "\n",
    "            # Loop through the div elements and extract the data\n",
    "            for div in div_elements:\n",
    "                # Extract the representative's name\n",
    "                name = div.find('span', class_='q-name').text.strip()\n",
    "\n",
    "                # Extract the political party\n",
    "                party = div.find('span', text='Parti authenticité et modernité')\n",
    "                if party:\n",
    "                    party = party.text.strip()\n",
    "                else:\n",
    "                    party = div.find('span', class_='q-name').find_next('span').text.strip()\n",
    "\n",
    "                # Extract the commission and group information\n",
    "                commission = div.find('a', href=True, text=True).text.strip()\n",
    "                group = div.find('a', href=True, text=True).find_next('a', href=True, text=True).text.strip()\n",
    "\n",
    "                # Write the data to the CSV file\n",
    "                csv_writer.writerow([name, party, commission, group])\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page {page_number}. Status code:\", response.status_code)\n",
    "\n",
    "print(\"Data has been scraped and stored in 'representatives.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdbf1062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\pc\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "665b22d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7352\\1377558934.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0minfo_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquestion_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'q-b3-col'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Date : '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mquestionneur\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Le questionneur'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Append data to the respective lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_next'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.chambredesrepresentants.ma/fr/questions-orales'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the question items\n",
    "    question_items = soup.find_all('div', class_='q-b3-item')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    questions = []\n",
    "    dates = []\n",
    "    questionneurs = []\n",
    "\n",
    "    # Loop through question items and extract data\n",
    "    for question_item in question_items:\n",
    "        # Extract the question text\n",
    "        question = question_item.find('span', text='Question  : ').find_next('a', href=True).text.strip()\n",
    "\n",
    "        # Extract the date and questionneur\n",
    "        info_div = question_item.find('div', class_='q-b3-col')\n",
    "        date = info_div.find('span', text='Date : ').find_next('span').text.strip()\n",
    "        questionneur = info_div.find('span', text='Le questionneur').find_next('a', href=True).text.strip()\n",
    "\n",
    "        # Append data to the respective lists\n",
    "        questions.append(question)\n",
    "        dates.append(date)\n",
    "        questionneurs.append(questionneur)\n",
    "\n",
    "    # Print or process the scraped data as needed\n",
    "    for i in range(len(questions)):\n",
    "        print(\"Question:\", questions[i])\n",
    "        print(\"Date:\", dates[i])\n",
    "        print(\"Questionneur:\", questionneurs[i])\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72754939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: تعزيز السيادة الدوائية ببلادنا\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: تعزيز السيادة الصحية ببلادنا\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: تقليـص الآجال المطلوبـة لدراسـة ملفـات منـح الإذن بالعـرض فـي السـوق بالنسبة لقطاع الأدوية\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: تعزيز السياسات العمومية في مجال الصحة العقلية\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: محاربة الوصم الاجتماعي للاضطرابات النفسية والانتحار\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: وضع برامج لدعم الوالدين في مجال تربية الأبناء\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: دعم أسر الأشخاص المصابين بالتوحد فيما يتعلق بكلفة العلاج\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: تحفيز الفلاحين على استعمال أصناف الحبوب الأكثر مقاومة للجفاف\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: تعزيز ميزانية البحث العلمي في المجال الفلاحي\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: إعادة توجيه الزراعات للحد من التصدير غير المباشر للمياه\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: التصدي للمتحورات الجديدة لفيروس لكورونا\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: استراتيجية الوزارة للتقليل من معدل وفيات الأمهات أثناء الحمل بالمناطق القروية والجبلية\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: تحسين خدمات الصحة الإنجابية بمؤسسات الرعاية الصحية الأولية\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: تأهيل اقسام الولادة بالمستشفيات الإقليمية\n",
      "Date: Date :\n",
      "Questionneur: Date :\n",
      "Question: تجويد الخدمات المقدمة بدور الطالب والطالبة بالمجالين القروي والجبلي\n",
      "Date: Date :\n",
      "Questionneur: Date :\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.chambredesrepresentants.ma/fr/questions-orales'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the question items\n",
    "    question_items = soup.find_all('div', class_='q-b3-item')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    questions = []\n",
    "    dates = []\n",
    "    questionneurs = []\n",
    "\n",
    "    # Loop through question items and extract data\n",
    "    for question_item in question_items:\n",
    "        # Extract the question text\n",
    "        question = question_item.find('a', href=True).text.strip()\n",
    "\n",
    "        # Extract the date and questionneur\n",
    "        info_div = question_item.find('div', class_='q-b3-col')\n",
    "        spans = info_div.find_all('span')\n",
    "        date = spans[-1].text.strip()  # Date is now at the last position\n",
    "        questionneur = spans[1].text.strip()  # Questionneur is now at the second position\n",
    "\n",
    "        # Append data to the respective lists\n",
    "        questions.append(question)\n",
    "        dates.append(date)\n",
    "        questionneurs.append(questionneur)\n",
    "\n",
    "    # Print or process the scraped data as needed\n",
    "    for i in range(len(questions)):\n",
    "        print(\"Question:\", questions[i])\n",
    "        print(\"Date:\", dates[i])\n",
    "        print(\"Questionneur:\", questionneurs[i])\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "908010ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'questions_data.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.chambredesrepresentants.ma/fr/questions-orales'\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the text blocks containing question details\n",
    "    question_blocks = soup.find_all('div', class_='q-b3-col')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    questions = []\n",
    "    dates = []\n",
    "    questionneurs = []\n",
    "\n",
    "    # Loop through question blocks and extract data\n",
    "    for question_block in question_blocks:\n",
    "        # Extract the question text\n",
    "        question = question_block.find('a', href=True).text.strip()\n",
    "\n",
    "        # Extract the date (if it exists)\n",
    "        date_element = question_block.find(string='Date : ')\n",
    "        date = date_element.find_next('div').text.strip() if date_element else 'N/A'\n",
    "\n",
    "        # Extract the questionneur (if it exists)\n",
    "        questionneur_element = question_block.find(string='Le questionneur')\n",
    "        questionneur = questionneur_element.find_next('a').text.strip() if questionneur_element else 'N/A'\n",
    "\n",
    "        # Append data to the respective lists\n",
    "        questions.append(question)\n",
    "        dates.append(date)\n",
    "        questionneurs.append(questionneur)\n",
    "\n",
    "    # Create a DataFrame to hold the data\n",
    "    data = {\n",
    "        'Question': questions,\n",
    "        'Date': dates,\n",
    "        'Questionneur': questionneurs\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df.to_excel('questions_data.xlsx', index=False)\n",
    "\n",
    "    print(\"Data saved to 'questions_data.xlsx'\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85638a54",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20204\\568288551.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Extract questioner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mquestion_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'questioner'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_block\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Le questionneur'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Extract relevant ministry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_next'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "url = \"https://www.chambredesrepresentants.ma/fr/questions-ecrites\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the 'q-block3' div elements\n",
    "    q_blocks = soup.find_all('div', class_='q-block3')\n",
    "\n",
    "    # Extract and print the data\n",
    "    for q_block in q_blocks:\n",
    "        question_data = {}\n",
    "\n",
    "        # Extract question text\n",
    "        question_data['question'] = q_block.find('span', text='Question  : ').find_next('a').text.strip()\n",
    "\n",
    "        # Extract date\n",
    "        question_data['date'] = q_block.find('span', text='Date : ').find_next('div').text.strip()\n",
    "\n",
    "        # Extract questioner\n",
    "        question_data['questioner'] = q_block.find('span', text='Le questionneur').find_next('a').text.strip()\n",
    "\n",
    "        # Extract relevant ministry\n",
    "        question_data['ministry'] = q_block.find('span', text='Le ministère compétent').find_next('div').text.strip()\n",
    "\n",
    "        print(\"Question:\", question_data['question'])\n",
    "        print(\"Date:\", question_data['date'])\n",
    "        print(\"Questioner:\", question_data['questioner'])\n",
    "        print(\"Ministry:\", question_data['ministry'])\n",
    "        print()\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "377664b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: مصير الطريق غير المرقمة الرابطة بين جماعة فيفي وجماعة بني فغلوم ومركز باب تازة بإقليم شفشاون\n",
      "Date: Le questionneur                      El Amine El Bakkali Tahiri\n",
      "\n",
      "\n",
      " Le ministère compétent    التجهيز والماء.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: إدماج مجموعة من ممرضي وتقنيي الصحة في الإطار العالي للصحة\n",
      "Date: Le questionneur                      Hassan Oumribte  \n",
      "\n",
      "\n",
      " Le ministère compétent    الصحة والحماية الاجتماعية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: سبل مكافحة المضاربة في المنتجات الغذائية الفلاحية\n",
      "Date: Le questionneur                      Hassan Oumribte  \n",
      "\n",
      "\n",
      " Le ministère compétent    الصناعة والتجارة.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: تيسير إجراءات دفن ضحايا الزلزال\n",
      "Date: Le questionneur                      Nadia El Kansouri \n",
      "\n",
      "\n",
      " Le ministère compétent    الداخلية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: الحركة الإنتقالية للأطباء الإختصاصيين برسم سنة2023\n",
      "Date: Le questionneur                      Nor-Ddin Elhrouchi  \n",
      "\n",
      "\n",
      " Le ministère compétent    الصحة والحماية الاجتماعية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: الوضعية  المهترئة لفرعية المهازيل التابعة لمجموعة مدارس الفقرة بجماعة الجعيدات باقليم الرحامنة\n",
      "Date: Le questionneur                      Abdellatif Zaim  \n",
      "\n",
      "\n",
      " Le ministère compétent    التربية الوطنية والتعليم الأولي والرياضة.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: بشأن توفير تسهيلات وتخفيضات لتنقل الطلبة الباحثين بين المدن عبر القطارات\n",
      "Date: Le questionneur                      Rim Chabat  \n",
      "\n",
      "\n",
      " Le ministère compétent    التعليم العالي والبحث العلمي والابتكار.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: بشأن الإجراءات المتخذة لتوفير بطاقة التخفيض للطلبة الباحثين على متن القطارات\n",
      "Date: Le questionneur                      Rim Chabat  \n",
      "\n",
      "\n",
      " Le ministère compétent    النقل واللوجيستيك.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: بيع أدوية تستعمل في الإجهاض على بعض المواقع بالأنترنيت بالمغرب\n",
      "Date: Le questionneur                      Abdellah Bouanou  \n",
      "\n",
      "\n",
      " Le ministère compétent    الصحة والحماية الاجتماعية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: الوضعية المزرية للطريق الإقليمية الرابطة بين جماعتي احرارة والبدوزة بإقليم آسفي، وأفق الإصلاح\n",
      "Date: Le questionneur                      Adil Essoubai  \n",
      "\n",
      "\n",
      " Le ministère compétent    التجهيز والماء.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: تدهور مستوى المعيشة للمواطنين جراء غلاء الأسعار\n",
      "Date: Le questionneur                      Driss Sentissi  \n",
      "\n",
      "\n",
      " Le ministère compétent    الاقتصاد والمالية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: انعكاس الارتفاعات المتكررة لأسعار المحروقات على القدرة الشرائية\n",
      "Date: Le questionneur                      Driss Sentissi  \n",
      "\n",
      "\n",
      " Le ministère compétent    الاقتصاد والمالية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: الحد من ارتفاع حوادث السير عبر الطرقات ببلادنا\n",
      "Date: Le questionneur                      Driss Sentissi  \n",
      "\n",
      "\n",
      " Le ministère compétent    النقل واللوجيستيك.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: الفوضى التي تعم بالقطار الرابط بين الدار البيضاء الميناء والجديدة\n",
      "Date: Le questionneur                      Hind Bennani  \n",
      "\n",
      "\n",
      " Le ministère compétent    النقل واللوجيستيك.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: المعاناة الشديدة لسكان العديد من مناطق البلاد من أزمة انعدام الماء الصالح للشرب\n",
      "Date: Le questionneur                      Touria Afif  \n",
      "\n",
      "\n",
      " Le ministère compétent    التجهيز والماء.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: النقل السري وفاجعة طريق دمنات\n",
      "Date: Le questionneur                      Fatima Zahra Bata \n",
      "\n",
      "\n",
      " Le ministère compétent    النقل واللوجيستيك.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: المعاناة الشديدة لسكان العديد من مناطق البلاد من أزمة انعدام الماء الصالح للشرب\n",
      "Date: Le questionneur                      Touria Afif  \n",
      "\n",
      "\n",
      " Le ministère compétent    الداخلية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: المعاناة الشديدة لسكان العديد من مناطق البلاد من أزمة انعدام الماء الصالح للشرب\n",
      "Date: Le questionneur                      Touria Afif  \n",
      "\n",
      "\n",
      " Le ministère compétent    الفلاحة والصيد البحري والتنمية القروية والمياه والغابات.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: عدم تفعيل قانون رقم 2216 المتعلق بالألعاب النارية\n",
      "Date: Le questionneur                      Hind Bennani  \n",
      "\n",
      "\n",
      " Le ministère compétent    الداخلية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: خطورة تزايد اختراق مضامين مدرسية منافية للثوابت الوطنية ومهددة للقيم الإنسانية النبيلة أمام ضعف الرقابة بغياب تكوين اللجنة الدائمة لملاءمة وتجديد البرامج والمناهج\n",
      "Date: Le questionneur                      Touria Afif  \n",
      "\n",
      "\n",
      " Le ministère compétent    التربية الوطنية والتعليم الأولي والرياضة.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: اختراق بعض المواقع الإلكترونية للكليات والجامعات ببلادنا\n",
      "Date: Le questionneur                      Rachid Hamouni  \n",
      "\n",
      "\n",
      " Le ministère compétent    التعليم العالي والبحث العلمي والابتكار.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: غياب التغطية بشبكة الهاتف والإنترنيت في مجموعة من دواوير جماعة أورير بإقليم أكادير إداوتنان\n",
      "Date: Le questionneur                      Hassan Oumribte  \n",
      "\n",
      "\n",
      " Le ministère compétent    الوزارة المنتدبة لدى رئيس الحكومة المكلفة بالانتقال الرقمي وإصلاح الإدارة.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: تقييم حصيلة المرحلة الثالثة للمبادرة الوطنية للتنمية البشرية على مستوى إقليم الخميسات\n",
      "Date: Le questionneur                      Nadia Touhami  \n",
      "\n",
      "\n",
      " Le ministère compétent    الداخلية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: اقتناء حواسيب بمساهمات انخراطات الشغيلة التعليمية بمؤسسة محمد السادس  للنهوض بالأعمال الاجتماعية للتربية والتكوين\n",
      "Date: Le questionneur                      Fatima Zahra Bata \n",
      "\n",
      "\n",
      " Le ministère compétent    التربية الوطنية والتعليم الأولي والرياضة.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: إلزام شركات الاتصالات باحترام الشروط التقنية عند تثبيت وتمرير أسلاك الأنترنيت والهاتف بالمنازل والمحلات التجارية بأحياء مدينة تطوان\n",
      "Date: Le questionneur                      Hamid Darrak  \n",
      "\n",
      "\n",
      " Le ministère compétent    الوزارة المنتدبة لدى رئيس الحكومة المكلفة بالانتقال الرقمي وإصلاح الإدارة.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: حماية الأنشطة الفلاحية بالقطاع الغابوي من الاندثار بإقليم الراشيدية\n",
      "Date: Le questionneur                      Samira Hijazi  \n",
      "\n",
      "\n",
      " Le ministère compétent    الفلاحة والصيد البحري والتنمية القروية والمياه والغابات.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: الإجراءات الوقائية لحماية الواحات من الحرائق\n",
      "Date: Le questionneur                      Naima El Fethaoui \n",
      "\n",
      "\n",
      " Le ministère compétent    الداخلية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: ارتفاع معدل الوفيات الناجمة عن لسعات العقارب ولدغات الأفاعي\n",
      "Date: Le questionneur                      Naima El Fethaoui \n",
      "\n",
      "\n",
      " Le ministère compétent    الصحة والحماية الاجتماعية.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: استعمال لغات أجنبية للتواصل مع الرأي العام الوطني وفي التراسل الإداري\n",
      "Date: Le questionneur                      Naima El Fethaoui \n",
      "\n",
      "\n",
      " Le ministère compétent    الفلاحة والصيد البحري والتنمية القروية والمياه والغابات.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n",
      "Question: قرية الخزف الولجة بسلا\n",
      "Date: Le questionneur                      Naima El Fethaoui \n",
      "\n",
      "\n",
      " Le ministère compétent    السياحة والصناعة التقليدية والاقتصاد الاجتماعي والتضامني.\n",
      "Questioner: Not available\n",
      "Ministry: Not available\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "url = \"https://www.chambredesrepresentants.ma/fr/questions-ecrites\"\n",
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06f75438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "url = \"https://www.chambredesrepresentants.ma/fr/questions-ecrites\"\n",
    "response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ad3c2",
   "metadata": {},
   "source": [
    "# les questions ecrites#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f1434880",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: مصير الطريق غير المرقمة الرابطة بين جماعة فيفي وجماعة بني فغلوم ومركز باب تازة بإقليم شفشاون\n",
      "Date: 14/09/2023\n",
      "Questioner: El Amine El Bakkali Tahiri\n",
      "Ministry: التجهيز والماء.\n",
      "\n",
      "\n",
      "Question: إدماج مجموعة من ممرضي وتقنيي الصحة في الإطار العالي للصحة\n",
      "Date: 14/09/2023\n",
      "Questioner: Hassan Oumribte\n",
      "Ministry: الصحة والحماية الاجتماعية.\n",
      "\n",
      "\n",
      "Question: سبل مكافحة المضاربة في المنتجات الغذائية الفلاحية\n",
      "Date: 14/09/2023\n",
      "Questioner: Hassan Oumribte\n",
      "Ministry: الصناعة والتجارة.\n",
      "\n",
      "\n",
      "Question: تيسير إجراءات دفن ضحايا الزلزال\n",
      "Date: 14/09/2023\n",
      "Questioner: Nadia El Kansouri\n",
      "Ministry: الداخلية.\n",
      "\n",
      "\n",
      "Question: الحركة الإنتقالية للأطباء الإختصاصيين برسم سنة2023\n",
      "Date: 14/09/2023\n",
      "Questioner: Nor-Ddin Elhrouchi\n",
      "Ministry: الصحة والحماية الاجتماعية.\n",
      "\n",
      "\n",
      "Question: الوضعية  المهترئة لفرعية المهازيل التابعة لمجموعة مدارس الفقرة بجماعة الجعيدات باقليم الرحامنة\n",
      "Date: 14/09/2023\n",
      "Questioner: Abdellatif Zaim\n",
      "Ministry: التربية الوطنية والتعليم الأولي والرياضة.\n",
      "\n",
      "\n",
      "Question: بشأن توفير تسهيلات وتخفيضات لتنقل الطلبة الباحثين بين المدن عبر القطارات\n",
      "Date: 14/09/2023\n",
      "Questioner: Rim Chabat\n",
      "Ministry: التعليم العالي والبحث العلمي والابتكار.\n",
      "\n",
      "\n",
      "Question: بشأن الإجراءات المتخذة لتوفير بطاقة التخفيض للطلبة الباحثين على متن القطارات\n",
      "Date: 14/09/2023\n",
      "Questioner: Rim Chabat\n",
      "Ministry: النقل واللوجيستيك.\n",
      "\n",
      "\n",
      "Question: بيع أدوية تستعمل في الإجهاض على بعض المواقع بالأنترنيت بالمغرب\n",
      "Date: 14/09/2023\n",
      "Questioner: Abdellah Bouanou\n",
      "Ministry: الصحة والحماية الاجتماعية.\n",
      "\n",
      "\n",
      "Question: الوضعية المزرية للطريق الإقليمية الرابطة بين جماعتي احرارة والبدوزة بإقليم آسفي، وأفق الإصلاح\n",
      "Date: 14/09/2023\n",
      "Questioner: Adil Essoubai\n",
      "Ministry: التجهيز والماء.\n",
      "\n",
      "\n",
      "Question: تدهور مستوى المعيشة للمواطنين جراء غلاء الأسعار\n",
      "Date: 14/09/2023\n",
      "Questioner: Driss Sentissi\n",
      "Ministry: الاقتصاد والمالية.\n",
      "\n",
      "\n",
      "Question: انعكاس الارتفاعات المتكررة لأسعار المحروقات على القدرة الشرائية\n",
      "Date: 14/09/2023\n",
      "Questioner: Driss Sentissi\n",
      "Ministry: الاقتصاد والمالية.\n",
      "\n",
      "\n",
      "Question: الحد من ارتفاع حوادث السير عبر الطرقات ببلادنا\n",
      "Date: 14/09/2023\n",
      "Questioner: Driss Sentissi\n",
      "Ministry: النقل واللوجيستيك.\n",
      "\n",
      "\n",
      "Question: الفوضى التي تعم بالقطار الرابط بين الدار البيضاء الميناء والجديدة\n",
      "Date: 14/09/2023\n",
      "Questioner: Hind Bennani\n",
      "Ministry: النقل واللوجيستيك.\n",
      "\n",
      "\n",
      "Question: المعاناة الشديدة لسكان العديد من مناطق البلاد من أزمة انعدام الماء الصالح للشرب\n",
      "Date: 14/09/2023\n",
      "Questioner: Touria Afif\n",
      "Ministry: التجهيز والماء.\n",
      "\n",
      "\n",
      "Question: النقل السري وفاجعة طريق دمنات\n",
      "Date: 14/09/2023\n",
      "Questioner: Fatima Zahra Bata\n",
      "Ministry: النقل واللوجيستيك.\n",
      "\n",
      "\n",
      "Question: المعاناة الشديدة لسكان العديد من مناطق البلاد من أزمة انعدام الماء الصالح للشرب\n",
      "Date: 14/09/2023\n",
      "Questioner: Touria Afif\n",
      "Ministry: الداخلية.\n",
      "\n",
      "\n",
      "Question: المعاناة الشديدة لسكان العديد من مناطق البلاد من أزمة انعدام الماء الصالح للشرب\n",
      "Date: 14/09/2023\n",
      "Questioner: Touria Afif\n",
      "Ministry: الفلاحة والصيد البحري والتنمية القروية والمياه والغابات.\n",
      "\n",
      "\n",
      "Question: عدم تفعيل قانون رقم 2216 المتعلق بالألعاب النارية\n",
      "Date: 14/09/2023\n",
      "Questioner: Hind Bennani\n",
      "Ministry: الداخلية.\n",
      "\n",
      "\n",
      "Question: خطورة تزايد اختراق مضامين مدرسية منافية للثوابت الوطنية ومهددة للقيم الإنسانية النبيلة أمام ضعف الرقابة بغياب تكوين اللجنة الدائمة لملاءمة وتجديد البرامج والمناهج\n",
      "Date: 14/09/2023\n",
      "Questioner: Touria Afif\n",
      "Ministry: التربية الوطنية والتعليم الأولي والرياضة.\n",
      "\n",
      "\n",
      "Question: اختراق بعض المواقع الإلكترونية للكليات والجامعات ببلادنا\n",
      "Date: 14/09/2023\n",
      "Questioner: Rachid Hamouni\n",
      "Ministry: التعليم العالي والبحث العلمي والابتكار.\n",
      "\n",
      "\n",
      "Question: غياب التغطية بشبكة الهاتف والإنترنيت في مجموعة من دواوير جماعة أورير بإقليم أكادير إداوتنان\n",
      "Date: 14/09/2023\n",
      "Questioner: Hassan Oumribte\n",
      "Ministry: الوزارة المنتدبة لدى رئيس الحكومة المكلفة بالانتقال الرقمي وإصلاح الإدارة.\n",
      "\n",
      "\n",
      "Question: تقييم حصيلة المرحلة الثالثة للمبادرة الوطنية للتنمية البشرية على مستوى إقليم الخميسات\n",
      "Date: 14/09/2023\n",
      "Questioner: Nadia Touhami\n",
      "Ministry: الداخلية.\n",
      "\n",
      "\n",
      "Question: اقتناء حواسيب بمساهمات انخراطات الشغيلة التعليمية بمؤسسة محمد السادس  للنهوض بالأعمال الاجتماعية للتربية والتكوين\n",
      "Date: 14/09/2023\n",
      "Questioner: Fatima Zahra Bata\n",
      "Ministry: التربية الوطنية والتعليم الأولي والرياضة.\n",
      "\n",
      "\n",
      "Question: إلزام شركات الاتصالات باحترام الشروط التقنية عند تثبيت وتمرير أسلاك الأنترنيت والهاتف بالمنازل والمحلات التجارية بأحياء مدينة تطوان\n",
      "Date: 14/09/2023\n",
      "Questioner: Hamid Darrak\n",
      "Ministry: الوزارة المنتدبة لدى رئيس الحكومة المكلفة بالانتقال الرقمي وإصلاح الإدارة.\n",
      "\n",
      "\n",
      "Question: حماية الأنشطة الفلاحية بالقطاع الغابوي من الاندثار بإقليم الراشيدية\n",
      "Date: 14/09/2023\n",
      "Questioner: Samira Hijazi\n",
      "Ministry: الفلاحة والصيد البحري والتنمية القروية والمياه والغابات.\n",
      "\n",
      "\n",
      "Question: الإجراءات الوقائية لحماية الواحات من الحرائق\n",
      "Date: 14/09/2023\n",
      "Questioner: Naima El Fethaoui\n",
      "Ministry: الداخلية.\n",
      "\n",
      "\n",
      "Question: ارتفاع معدل الوفيات الناجمة عن لسعات العقارب ولدغات الأفاعي\n",
      "Date: 14/09/2023\n",
      "Questioner: Naima El Fethaoui\n",
      "Ministry: الصحة والحماية الاجتماعية.\n",
      "\n",
      "\n",
      "Question: استعمال لغات أجنبية للتواصل مع الرأي العام الوطني وفي التراسل الإداري\n",
      "Date: 14/09/2023\n",
      "Questioner: Naima El Fethaoui\n",
      "Ministry: الفلاحة والصيد البحري والتنمية القروية والمياه والغابات.\n",
      "\n",
      "\n",
      "Question: قرية الخزف الولجة بسلا\n",
      "Date: 14/09/2023\n",
      "Questioner: Naima El Fethaoui\n",
      "Ministry: السياحة والصناعة التقليدية والاقتصاد الاجتماعي والتضامني.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send an HTTP GET request to the webpage\n",
    "url = 'https://www.chambredesrepresentants.ma/fr/questions-ecrites'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the div elements with class \"q-bloack3\"\n",
    "    q_blocks = soup.select('div.q-block3')\n",
    "\n",
    "    # Loop through the div elements and extract the data\n",
    "    for q_block in q_blocks:\n",
    "        # Extract the question text if available, or set it to \"N/A\" if not found\n",
    "        question = q_block.find('span', text='Question  : ').find_next('a').text.strip()\n",
    "\n",
    "        # Extract the date if available, or set it to \"N/A\" if not found\n",
    "        ministry_element = soup.find('div', {'class': 'q-b3-col'}).find_all('div')[1]\n",
    "        date = ministry_element.text.strip().replace('Date :', '').strip()  # Remove \"Date :\" prefix\n",
    "\n",
    "        # Extract the questioner if available, or set it to \"N/A\" if not found\n",
    "        questioner_element = q_block.select_one('div.q-b3-col a[href*=\"/m/\"]')\n",
    "        questioner = questioner_element.text.strip() if questioner_element else 'N/A'\n",
    "\n",
    "        # Extract the relevant ministry if available, or set it to \"N/A\" if not found\n",
    "        div_elements = q_block.select('div.q-b3-col')\n",
    "        for div in div_elements:\n",
    "            # Get the text content of all divs within this div\n",
    "            div_text = ' '.join([item.strip() for item in div.stripped_strings])\n",
    "\n",
    "            # Check if \"Le ministère compétent\" is in the text\n",
    "            if 'Le ministère compétent' in div_text:\n",
    "                # Extract the ministry name by splitting the text on the label\n",
    "                ministry = div_text.split('Le ministère compétent')[-1].strip()\n",
    "                # Print the extracted data\n",
    "                print(\"Question:\", question)\n",
    "                print(\"Date:\", date)\n",
    "                print(\"Questioner:\", questioner)\n",
    "                print(\"Ministry:\", ministry)\n",
    "                print(\"\\n\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5a18f5fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to 'scraped_data.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://www.chambredesrepresentants.ma/fr/questions-ecrites?page={}'\n",
    "\n",
    "# Define the number of pages you want to scrape (adjust as needed)\n",
    "num_pages = 50\n",
    "\n",
    "# Create empty lists to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through the pages\n",
    "for page_number in range(1, num_pages + 1):\n",
    "    # Create the complete URL for the current page\n",
    "    url = base_url.format(page_number)\n",
    "\n",
    "    # Send an HTTP GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the div elements with class \"q-bloack3\"\n",
    "        q_blocks = soup.select('div.q-block3')\n",
    "\n",
    "        # Loop through the div elements and extract the data\n",
    "        for q_block in q_blocks:\n",
    "            # Extract the question text if available, or set it to \"N/A\" if not found\n",
    "            question = q_block.find('span', text='Question  : ').find_next('a').text.strip()\n",
    "\n",
    "            # Extract the date if available, or set it to \"N/A\" if not found\n",
    "            ministry_element = soup.find('div', {'class': 'q-b3-col'}).find_all('div')[1]\n",
    "            date = ministry_element.text.strip().replace('Date :', '').strip()  # Remove \"Date :\" prefix\n",
    "\n",
    "            # Extract the questioner if available, or set it to \"N/A\" if not found\n",
    "            questioner_element = q_block.select_one('div.q-b3-col a[href*=\"/m/\"]')\n",
    "            questioner = questioner_element.text.strip() if questioner_element else 'N/A'\n",
    "\n",
    "            # Extract the relevant ministry if available, or set it to \"N/A\" if not found\n",
    "            div_elements = q_block.select('div.q-b3-col')\n",
    "            for div in div_elements:\n",
    "                # Get the text content of all divs within this div\n",
    "                div_text = ' '.join([item.strip() for item in div.stripped_strings])\n",
    "\n",
    "                # Check if \"Le ministère compétent\" is in the text\n",
    "                if 'Le ministère compétent' in div_text:\n",
    "                    # Extract the ministry name by splitting the text on the label\n",
    "                    ministry = div_text.split('Le ministère compétent')[-1].strip()\n",
    "                    # Append the data to the list\n",
    "                    data.append([question, date, questioner, ministry])\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from page {page_number}. Status code:\", response.status_code)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "columns = ['Question', 'Date', 'Questioner', 'Ministry']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel('scraped_data.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been successfully saved to 'scraped_data.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70527dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://www.chambredesrepresentants.ma/fr/questions-ecrites?page={}'\n",
    "\n",
    "# Define the number of pages you want to scrape (adjust as needed)\n",
    "num_pages = 100\n",
    "\n",
    "# Create empty lists to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through the pages\n",
    "for page_number in range(1, num_pages + 1):\n",
    "    # Create the complete URL for the current page\n",
    "    url = base_url.format(page_number)\n",
    "\n",
    "    # Send an HTTP GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the div elements with class \"q-bloack3\"\n",
    "        q_blocks = soup.select('div.q-block3')\n",
    "\n",
    "        # Loop through the div elements and extract the data\n",
    "        for q_block in q_blocks:\n",
    "            # Extract the question text if available, or set it to \"N/A\" if not found\n",
    "            question = q_block.find('span', text='Question  : ').find_next('a').text.strip()\n",
    "\n",
    "            # Extract the date if available, or set it to \"N/A\" if not found\n",
    "            ministry_element = soup.find('div', {'class': 'q-b3-col'}).find_all('div')[1]\n",
    "            date = ministry_element.text.strip().replace('Date :', '').strip()  # Remove \"Date :\" prefix\n",
    "\n",
    "            # Extract the questioner if available, or set it to \"N/A\" if not found\n",
    "            questioner_element = q_block.select_one('div.q-b3-col a[href*=\"/m/\"]')\n",
    "            questioner = questioner_element.text.strip() if questioner_element else 'N/A'\n",
    "\n",
    "            # Extract the relevant ministry if available, or set it to \"N/A\" if not found\n",
    "            div_elements = q_block.select('div.q-b3-col')\n",
    "            for div in div_elements:\n",
    "                # Get the text content of all divs within this div\n",
    "                div_text = ' '.join([item.strip() for item in div.stripped_strings])\n",
    "\n",
    "                # Check if \"Le ministère compétent\" is in the text\n",
    "                if 'Le ministère compétent' in div_text:\n",
    "                    # Extract the ministry name by splitting the text on the label\n",
    "                    ministry = div_text.split('Le ministère compétent')[-1].strip()\n",
    "                    # Append the data to the list\n",
    "                    data.append([question, date, questioner, ministry])\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from page {page_number}. Status code:\", response.status_code)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "columns = ['Question', 'Date', 'Questioner', 'Ministry']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('questions_ecrites.csv', index=False)\n",
    "\n",
    "print(\"Data has been successfully saved to 'questions_ecrites.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b25ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '''\n",
    "<div class=\"q-block3\">\n",
    "    <div class=\"q-b3-item  q-b3i-red \">\n",
    "        <span class=\"q-status  q-st-red \"></span>\n",
    "        <div class=\"q-b3-col\">\n",
    "            <div>\n",
    "                <span>Question  : </span> <a href=\"/fr/%D9%85%D8%B1%D8%A7%D9%82%D8%A8%D8%A9-%D8%A7%D9%84%D8%B9%D9%85%D9%84-%D8%A7%D9%84%D8%AD%D9%83%D9%88%D9%85%D9%8A/%D8%A7%D9%84%D8%A3%D8%B3%D8%A6%D9%84%D8%A9-%D8%A7%D9%84%D8%B4%D9%81%D9%88%D9%8A%D8%A9/%D8%AA%D9%82%D9%84%D9%8A%D8%B5-%D8%A7%D9%84%D8%A2%D8%AC%D8%A7%D9%84-%D8%A7%D9%84%D9%85%D8%B7%D9%84%D9%88%D8%A8%D9%80%D8%A9-%D9%84%D8%AF%D8%B1%D8%A7%D8%B3%D9%80%D8%A9-%D9%85%D9%84%D9%81%D9%80%D8%A7%D8%AA-%D9%85%D9%86%D9%80%D8%AD-%D8%A7%D9%84%D8%A5%D8%B2%D9%86-%D8%A8%D8%A7%D9%84%D8%B9%D8%B1%D8%B6-%D9%81%D9%80%D9%8A\">تقليـص الآجال المطلوبـة لدراسـة ملفـات منـح الإذن بالعـرض فـي السـوق بالنسبة لقطاع الأدوية</a>\n",
    "            </div>\n",
    "            <div>\n",
    "                <span>Date : </span>18/09/2023\n",
    "            </div>\n",
    "        </div>\n",
    "        <div class=\"q-b3-col\">\n",
    "            <div>\n",
    "                <span>Le questionneur</span> <a href=\"/fr/m/atouizi\">Ahmed Touizi  </a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Extract the question text\n",
    "question = soup.find('span', text='Question  : ').find_next('a').text.strip()\n",
    "\n",
    "# Extract the date\n",
    "date = soup.find('span', text='Date : ').next_sibling.strip()\n",
    "\n",
    "# Extract the questioner\n",
    "questioner = soup.find('span', text='Le questionneur').find_next('a').text.strip()\n",
    "\n",
    "# Print the extracted data\n",
    "print(\"Question:\", question)\n",
    "print(\"Date:\", date)\n",
    "print(\"Questioner:\", questioner)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c255c8",
   "metadata": {},
   "source": [
    "# questions orales#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3510472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: تعزيز السيادة الدوائية ببلادنا\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: تعزيز السيادة الصحية ببلادنا\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: تقليـص الآجال المطلوبـة لدراسـة ملفـات منـح الإذن بالعـرض فـي السـوق بالنسبة لقطاع الأدوية\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: تعزيز السياسات العمومية في مجال الصحة العقلية\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: محاربة الوصم الاجتماعي للاضطرابات النفسية والانتحار\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: وضع برامج لدعم الوالدين في مجال تربية الأبناء\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: دعم أسر الأشخاص المصابين بالتوحد فيما يتعلق بكلفة العلاج\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: تحفيز الفلاحين على استعمال أصناف الحبوب الأكثر مقاومة للجفاف\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: تعزيز ميزانية البحث العلمي في المجال الفلاحي\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: إعادة توجيه الزراعات للحد من التصدير غير المباشر للمياه\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: التصدي للمتحورات الجديدة لفيروس لكورونا\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: استراتيجية الوزارة للتقليل من معدل وفيات الأمهات أثناء الحمل بالمناطق القروية والجبلية\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: تحسين خدمات الصحة الإنجابية بمؤسسات الرعاية الصحية الأولية\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: تأهيل اقسام الولادة بالمستشفيات الإقليمية\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n",
      "Question: تجويد الخدمات المقدمة بدور الطالب والطالبة بالمجالين القروي والجبلي\n",
      "Date: 18/09/2023\n",
      "Questioner: Ahmed Touizi\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send an HTTP GET request to the webpage\n",
    "url = 'https://www.chambredesrepresentants.ma/fr/questions-orales'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the div elements with class \"q-bloack3\"\n",
    "    q_blocks = soup.select('div.q-block3')\n",
    "\n",
    "    # Loop through the div elements and extract the data\n",
    "    for q_block in q_blocks:\n",
    "        # Extract the question text if available, or set it to \"N/A\" if not found\n",
    "        question = q_block.find('span', text='Question  : ').find_next('a').text.strip()\n",
    "\n",
    "        # Extract the date if available, or set it to \"N/A\" if not found\n",
    "        ministry_element = soup.find('div', {'class': 'q-b3-col'}).find_all('div')[1]\n",
    "        date = ministry_element.text.strip().replace('Date :', '').strip()  # Remove \"Date :\" prefix\n",
    "\n",
    "        # Extract the questioner if available, or set it to \"N/A\" if not found\n",
    "        questioner_element = q_block.select_one('div.q-b3-col a[href*=\"/m/\"]')\n",
    "        questioner = questioner_element.text.strip() if questioner_element else 'N/A'\n",
    "\n",
    "        # Print the extracted data\n",
    "        print(\"Question:\", question)\n",
    "        print(\"Date:\", date)\n",
    "        print(\"Questioner:\", questioner)\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafd73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://www.chambredesrepresentants.ma/fr/questions-orales?page={}'\n",
    "\n",
    "# Define the number of pages you want to scrape (adjust as needed)\n",
    "num_pages = 500\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through the pages\n",
    "for page_number in range(1, num_pages + 1):\n",
    "    # Create the complete URL for the current page\n",
    "    url = base_url.format(page_number)\n",
    "\n",
    "    # Send an HTTP GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the div elements with class \"q-block3\"\n",
    "        q_blocks = soup.select('div.q-block3')\n",
    "\n",
    "        # Loop through the div elements and extract the data\n",
    "        for q_block in q_blocks:\n",
    "            # Extract the question text if available, or set it to \"N/A\" if not found\n",
    "            question = q_block.find('span', text='Question  : ').find_next('a').text.strip()\n",
    "\n",
    "            # Extract the date if available, or set it to \"N/A\" if not found\n",
    "            ministry_element = soup.find('div', {'class': 'q-b3-col'}).find_all('div')[1]\n",
    "            date = ministry_element.text.strip().replace('Date :', '').strip()  # Remove \"Date :\" prefix\n",
    "\n",
    "            # Extract the questioner if available, or set it to \"N/A\" if not found\n",
    "            questioner_element = q_block.select_one('div.q-b3-col a[href*=\"/m/\"]')\n",
    "            questioner = questioner_element.text.strip() if questioner_element else 'N/A'\n",
    "\n",
    "            # Append the data to the list\n",
    "            data.append([question, date, questioner])\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from page {page_number}. Status code:\", response.status_code)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "columns = ['Question', 'Date', 'Questioner']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel('orales.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been successfully saved to 'orales.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca2dc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from page 54. Status code: 520\n",
      "Data has been successfully saved to 'oralesComplet.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://www.chambredesrepresentants.ma/fr/questions-orales?page={}'\n",
    "\n",
    "# Define the number of pages you want to scrape (adjust as needed)\n",
    "num_pages = 544\n",
    "\n",
    "\n",
    "# Create an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through the pages\n",
    "for page_number in range(1, num_pages + 1):\n",
    "    # Create the complete URL for the current page\n",
    "    url = base_url.format(page_number)\n",
    "\n",
    "    # Send an HTTP GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the div elements with class \"q-block3\"\n",
    "        q_blocks = soup.select('div.q-block3')\n",
    "\n",
    "        # Loop through the div elements and extract the data\n",
    "        for q_block in q_blocks:\n",
    "            # Extract the question text if available, or set it to \"N/A\" if not found\n",
    "            question = q_block.find('span', text='Question  : ').find_next('a').text.strip()\n",
    "\n",
    "            # Extract the date if available, or set it to \"N/A\" if not found\n",
    "            ministry_element = soup.find('div', {'class': 'q-b3-col'}).find_all('div')[1]\n",
    "            date = ministry_element.text.strip().replace('Date :', '').strip()  # Remove \"Date :\" prefix\n",
    "\n",
    "            # Extract the questioner if available, or set it to \"N/A\" if not found\n",
    "            questioner_element = q_block.select_one('div.q-b3-col a[href*=\"/m/\"]')\n",
    "            questioner = questioner_element.text.strip() if questioner_element else 'N/A'\n",
    "\n",
    "            # Append the data to the list\n",
    "            data.append([question, date, questioner])\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from page {page_number}. Status code:\", response.status_code)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "columns = ['Question', 'Date', 'Questioner']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('oralesComplet.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Data has been successfully saved to 'oralesComplet.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14773ea0",
   "metadata": {},
   "source": [
    "# Combining data: If you have data in multiple CSV files, merge them into a unified dataset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c99cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
